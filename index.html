<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="VideoWebArena is a novel, open-source video-based benchmark that evaluates multimodal models' agentic ability to process, understand, and utilize long-context video inputs to accomplish various web tasks.">
  <meta name="keywords" content="videowebarena, webarena, ai agent, llm benchmark, ai benchmark, ai benchmarking, ai research, agentic">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .responsive-container {
        display: flex;
        justify-content: space-between;
        align-items: center;
        width: 80%; /* Adjust the width as needed */
        margin: 0 auto;
    }

    .banner-image {
        display: block;
        width: 140px;
    }

    .content-container {
        display: flex;
        flex-direction: column;
        align-items: flex-end;
    }

    .text-container {
        min-width: fit-content;
        text-align: center;
        flex: 1;
    }

    .microsoft-authors {
        margin-left: 140px;
        margin-right: 140px
    }

    @media (min-width: 768px) {
        .responsive-container {
            justify-content: center;
        }

        .content-container {
            flex-direction: row;
            justify-content: center; /* Center horizontally */
            align-items: center;
            width: 80%; /* Adjust as needed */
        }

        .banner-image {
            margin: 0;
        }

        .text-container {
            text-align: center;
            margin-left: 20px;
            min-width: fit-content;
        }

        .microsoft-authors {
            margin-left: 140px;
            margin-right: 140px;
        }
    }

    @media (max-width: 768px) {
        .responsive-container {
            flex-direction: column;
            align-items: center;
        }

        .content-container {
            flex-direction: column;
            justify-content: center; /* Center vertically */
            align-items: center;
            width: 80%; /* Adjust as needed */
        }

        .title.is-1 {
            font-size: 1.8rem; /* Smaller font size for mobile */
        }

        .subtitle.is-4 {
            font-size: 1.2rem; /* Smaller font size for mobile */
        }

        .banner-image {
            margin-bottom: 20px; /* Add space between the image and the text */
            width: 40%; /* Full width */
            margin-bottom: 0px;
        }

        .microsoft-authors {
            margin-left: 0px;
            margin-right: 0px;
        }
    }
</style>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <div class="responsive-container">
                        <div class="content-container">
                          <div class="text-container">
                            <h1 class="title is-1" style="word-break: keep-all;">VideoWebArena</h1>
                            <h2 class="subtitle is-4" style="word-break: keep-all;">Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</h2>
                        </div>
                                              
            </div>
            </div>
            <div style="margin-top: 15px;" class="is-size-5 publication-authors microsoft-authors">
              <span class="author-block">
                  <a href="http://lawrencekjang.github.io">Lawrence Jang</a><sup>1,4</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.microsoft.com/applied-sciences/people/yinheng-li">Yinheng Li</a><sup>4</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.linkedin.com/in/charles-dingg/">Charles Ding</a><sup>1</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.linkedin.com/in/justin-lin-68a0831a1/">Justin Lin</a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                  <a href="https://pliang279.github.io/">Paul Liang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.microsoft.com/applied-sciences/people/danny-zhao"> Dan Zhao</a><sup>2,3,4</sup>,
              </span>
              <span class="author-block">
                  <a href="http://rogeriobonatti.com/">Rogerio Bonatti</a><sup>4</sup>,
              </span>
              <span class="author-block">
                  <a href="https://www.microsoft.com/applied-sciences/people/kazuhito-koishida">Kazuhito Koishida</a><sup>4</sup>
              </span>
              <br>
              <div style="margin-top:5px;">
                  <sup>1</sup>Carnegie Mellon University, <sup>2</sup>Massachusetts Institute of Technology,<br>
                  <sup>3</sup>New York University, <sup>4</sup>Microsoft
              </div>
              <div style="margin-top: 3px; font-family: monospace; color: #888; font-size: 15px;">
                  ljang@cs.cmu.edu
            </div>
          </div>
                  
        <div style="margin-top: 3px;" class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2410.19100"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2410.19100"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/ljang0/videowebarena"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <span class="link-block">
              <a href="https://docs.google.com/spreadsheets/d/1mcZ77YDwvtYhusJzBTZpkiVtC60xNM3zeLWeFp_C6gc/edit?usp=sharing" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-trophy"></i> <!-- Trophy icon -->
                </span>
                <span>Leaderboard</span>
              </a>
            </span>
            
            <hr class="light-hr" style="margin-bottom: 0;">

          </div>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser" style="margin-top: 0px; margin-bottom: 0px;  padding-bottom: 0;">
  <div class="container is-max-desktop" style="margin-top: 0px; margin-bottom: 0px;  padding-bottom: 0;">
    <div class="hero-body">
      <img src="./static/images/headerFinal.png" alt="Header Image" style="width: 100%; margin-bottom: 0; ">
    </div>
  </div>
</section>

<section class="section" style="margin-top: 0px; padding-top: 0;">
  <div class="container is-max-desktop" style="margin-top: 0px; padding-top: 0;">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <!-- <h2 class="title is-3">Windows Agent Arena benchmark</h2> -->
        <div class="content has-text-justified">
          <p>Humans often use videos to complete daily tasks, whether to learn from tutorials or retrieve information from within one or several videos. As we build AI assistants, these multimodal agents must also
            possess similar capabilities to understand and process videos to accomplish tasks or learn how to
            accomplish a workflow, plan, and make decisions.</p>
          <p>However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs.</p>
          <p>To address this gap, we present <strong>VideoWebArena</strong>, a novel, open-source video-based benchmark that evaluates multimodal models' agentic ability to process, understand, and utilize long-context video inputs to accomplish various tasks. </p>
          <p>VideoWebArena consists of <strong>2,021 web agent
            tasks</strong> based on <strong>74 manually crafted video tutorials</strong>, which total almost four hours of
            content. For our benchmark, we define a taxonomy of long-context video-based
            agent tasks with two main areas of focus: <strong>skill retention and factual retention</strong>.
            While skill retention tasks evaluate whether an agent can use a given human demon-
            stration to complete a task efficiently, the factual retention task evaluates whether
            an agent can retrieve instruction-relevant information from a video to complete
            a task.</p>
          <p>We evaluate several video-capable state-of-the-art LLMs, namely GPT-4o and Gemini 1.5
            Pro, on our benchmark, providing an overview of these models' current long-context video
            understanding capabilities. Our results show that while these models can serve in a limited
            capacity as video-capable agents, these models are still a far reach from human levels of
            performance, highlighting a wide gap in the information retrieval and agentic abilities of
            current state-of-the-art long-context models.</p>
          <p>Our work
            highlights the need to improve the agentic abilities of long-context multimodal
            models and provides a testbed for future development with long-context video
            agents.</p>

          <!-- <p>Code and baseline models: <a href="https://github.com/microsoft/WindowsAgentArena">github.com/microsoft/WindowsAgentArena</a>.</p> -->
        </div>
  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/edge_track.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vscode.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vlc.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bing.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/edge.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/paint.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/main.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section" style="margin-top: 0px;">
  <div class="container is-max-desktop" style="margin-top: 0px;">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Environment and Tasks</h2>
        <div class="content has-text-justified">
          <p>
            VideoWebArena centers around six key thematic environments created by VisualWebArena and WebArena: <strong>Reddit, Classifieds, Shopping, Shopping Admin, Map, and Gitlab</strong>. These domains' websites are locally hosted as the docker images for each website are publicly available online. There is an Amazon Machine Image and instructions dedicated to hosting these websites on an EC2 instance; we refer readers to the codebase for further information. By doing this, we can make our benchmark realistic and reproducible, leveraging data and code from real and popular websites on the internet. We refer to WebArena and VisualWebArena for more information on each site and their setup.
          </p>
          <p>
            The authors of this paper created <strong>74 unique videos</strong> to pair with <strong>2,021 tasks</strong>, totaling almost <strong>4 hours</strong>of video content ---all of our videos are tutorials based on tasks in WebArena and VisualWebArena. We provide our videos online through a <a href = https://www.youtube.com/@webarenawarrior>Youtube Channel</a> and a <a href = https://drive.google.com/file/d/17DwmsM7KzBWyz1BN1aq7NHDvgcTIrCgx/view>Google Drive link</a> containing the zip file of all the videos. We formulated these videos by accumulating all the feasible tasks in WebArena and VisualWebArena, and then filming task tutorials on our personal computers. 
          </p>

          <p>We provide a breakdown of the videos and tasks below. More information about the task taxonomy and difficulties can be found in the following sections.</p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <img src="./static/images/taskNew.png" height="240" alt="Tasks">
    </div>
<!-- 
    <div class="content has-text-justified">

      <p>
        Task evaluation is deterministic, and we use custom scripts to generate a reward at the end of each episode:
      </p>
    </div>

    <div class="content has-text-centered">
      <img src="./static/images/eval.png" height="200" alt="Eval">
    </div>
 -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Task Taxonomy: Skill and Factual Retention</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
            The taxonomy covers two subsets of tasks --- <strong>skill retention and factual retention</strong>--- inspired by real-world use cases. 
            <ul>
              <li><strong>Skill Retention Tasks</strong> test the ability to learn from and use a given human demonstration to efficiently complete a task. For example, using YouTube tutorials or screen recordings of expert demonstrations to learn how to perform a task is a form of skill retention.</li>
              <li><strong>Factual Retention Tasks</strong> test the ability to retrieve information relevant to a user's specific question/task present in a video that may not be the video's main focus (e.g., an incidental detail). 
                For example, one might want to buy the shoes a particular NBA player is wearing that are shown within a short duration of a much longer basketball highlights video. </li>
            </ul>
          </p>
          <p>
            We map each of our video tutorials to the respective tasks in the WebArena and VisualWebArena task set to create 1,621 skill retention tasks. Thus, each WA/VWA task has an expert human demonstration attached to it. An agent should be able to process the video tutorial and complete the task at hand at a higher rate than without the video.
          </p>
          <p>We created <strong>400 original factual retention tasks </strong> based on these
            same tutorials. The authors of each factual retention task also are tasked with creating <strong>intermediate intents</strong> that test if the model can extract the information necessary to complete the
            task. Therefore, each factual retention task has a <strong>Q/A pair</strong>, where the agent is prompted with the video and a task-related question. This serves as a intermediate checkpoint to evaluate the model's ability to process information from the video, unrelated to generating actions or acting in the environment. Skill Retention tasks do not have intermediate intents as we deemed learning from human demonstrations do not requires needle-in-the-haystack type of information retrieval.</p>

          <p>
            Furthermore, we define 4 types of factual retention tasks below:
          </p>
          <ul>
            <li><strong>Visual
              Perception (Tasks that require OCR, Spatial Reasoning)</strong></li>
            <li><strong>Audio Perception</strong></li>
            <li><strong>Full Video Understanding (i.e., tasks
              that require information across several parts of the video)</strong></li>
            <li><strong>Temporal Reasoning (i.e., tasks
              that require understanding the video with respect to time)</strong></li>
          </ul>

          <p>We provide examples of each task in our defined taxonomy below.</p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <img src="./static/images/examples_refit.png" height="40" alt="Tasks">
    </div>

    <p>We also define an agentic and video difficulty for each factual retention task. The <strong>agentic difficulty</strong> for each task signifies the complexity of the
      action sequence needed to complete an intent successfully. For agentic difficulty, we classify a task
      as easy if it can be completed in 1-3 steps, medium if it can be completed in 4-9 steps, and hard if it
      can be completed in more than 9 steps. This classification is adopted from VisualWebArena.</p>
    <br>
    <p>We provide <strong>video difficulty</strong> ratings for all intermediate intents, distributed between easy,
      medium, and hard. The video difficulty ratings signify the complexity of returning the correct answer
      for a given task's intermediate intent. Easy tasks require returning one piece of information and can
      be solved with less than 3 frames, medium tasks require returning 2 to 3 things and can be solved
      with less than half the video, and hard tasks require returning more than 3 things and require watching
      more than half the video. </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Baseline Agents</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
            We define a set of baseline agents using proprietary LLMs as the backbone. At each step, the agent is given the task objective, 2 in-context examples, current state (defined by Set-of-Marks), and the input video to the objective as context to generate one action. This follows the Set-of-Marks agent proposed in VisualWebArena.
          </p>
          <ul>
            <li><strong>Video In-Context Agent</strong>: An agent that takes the MP4 tutorial in-context (only available with Gemini models).</li>
            <li><strong>Video Frames In-Context Agent</strong>: An agent that takes evenly split frames and the transcripted audio in-context.</li>
            <li><strong>Video Summary Agent</strong>: An agent that takes no visual modality as input. The agent takes a summary of the video generated before the task in-context.</li>
          </ul>
          <p>
          Below you can see the framework for our baseline POMDP video agents.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/agent_refit.png" height="15" alt="Agent Example">
          </div>
          <p>We acknowledge the simple nature of our baseline agents. We hope our work encourages further work in this area and improvements in video agent architecture.</p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
          We benchmark serveral versions of the baseline agents we defined in the previous section along with human evaluation scores. We present the performances on the <strong>skill retention</strong> tasks below. Human performance shows tutorials should
          help task performance success and efficiency. However, adding tutorials in-context to the model does
          not necessarily help, but in fact hurts performance by a significant margin. This points to the idea that long-context agents cannot process the video information which creates negative noise while generating actions.
          </p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <!-- insert image -->
      <img src="./static/images/skill_refit.png" height="200" alt="Results">
    </div>

    <p>We provice the evaluation results on our set of <strong>factual retention</strong> tasks below. We note several consistent trends across LLM
      agent results. There is no winning baseline agent or model family across skill and factual retention
      tasks. For factual retention tasks, the summary agent performs the best in task success at <strong>13.3%</strong> while the 30 and 100 Frame GPT-4o Agent perform the best in intermediate intent success at <strong>45.8%</strong>, far below human performance
      at <strong>73.9%</strong> and <strong>79.3%</strong>, respectively.</p>
    <br>
    <p>Although intermediate scores tend to be higher than final scores, this does not necessarily translate to task success. This is a constant failure mode of the long-context agents, as
      they can perform the necessary VQA to extract the necessary information for the task at hand but fall
      short due to hallucinations, action grounding, and high-level planning errors.</p>
    <br>
    <div class="content has-text-centered">
      <!-- insert image -->
      <img src="./static/images/fac_refit.png" height="200" alt="Results">
    </div>

    <p>We provide further results on our factual retention task set with respect to <strong>task difficulties and types</strong> below. We see the summary agent has
      the best task performance, even without having any visual aspect of the video in context. However, it
      lags behind in the intermediate VQA intents, as the video frame and video agents all perform very
      similarly better on intermediate tasks.</p>
    <br>
    <div class="content has-text-centered">
      <!-- insert image -->
      <img src="./static/images/tasks_redit3.png" height="200" alt="Results">
    </div>

    <p>According to our experiments, the baseline agents do not perform
      well on most of tasks compared with human performance. There is still a long way in developing
      intelligent agents. For future work, it is important to analyze the failure cases explore better video
      agent architectures with different LLMs on this benchmark. We hope our environment and benchmark
      facilitate improvement and additional work on improving long-context multimodal agents.</p>

  </div>
</section>

<section class="section" id="Citations">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX citation</h2>
    <pre><code>@article{videowebarena,
      title={VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks},
      author={Lawrence Keunho Jang, Yinheng Li, Charles Ding, Justin Lin, Paul Pu Liang, Dan Zhao, Rogerio Bonatti, Kazuhito Koishida},
      journal={arXiv preprint},
      year={2024}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/files/windows_agent_arena.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/ljang/videowebarena" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
