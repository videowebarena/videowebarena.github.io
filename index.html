<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Windows Agent Arena (WAA) is a scalable Windows AI agent platform for testing and benchmarking multi-modal, desktop AI agents. WAA provides researchers and developers with a reproducible and realistic Windows OS environment for AI research, where agentic AI workflows can be tested across a diverse range of tasks. WAA supports the deployment of agents at scale using the Azure ML cloud infrastructure, allowing for the parallel running of multiple agents and delivering quick benchmark results for hundreds of tasks in minutes, not days.">
  <meta name="keywords" content="windows agent arena, windows arena, windows ai agent, ai agent, llm benchmark, ai benchmark, ai benchmarking, ai research, agentic">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .responsive-container {
        display: flex;
        justify-content: space-between;
        align-items: center;
        width: 80%; /* Adjust the width as needed */
        margin: 0 auto;
    }

    .banner-image {
        display: block;
        width: 140px;
    }

    .content-container {
        display: flex;
        flex-direction: column;
        align-items: flex-end;
    }

    .text-container {
        min-width: fit-content;
        text-align: center;
        flex: 1;
    }

    .microsoft-authors {
        margin-left: 140px;
        margin-right: 140px
    }

    @media (min-width: 768px) {
        .responsive-container {
            justify-content: center;
        }

        .content-container {
            flex-direction: row;
            justify-content: center; /* Center horizontally */
            align-items: center;
            width: 80%; /* Adjust as needed */
        }

        .banner-image {
            margin: 0;
        }

        .text-container {
            text-align: center;
            margin-left: 20px;
            min-width: fit-content;
        }

        .microsoft-authors {
            margin-left: 140px;
            margin-right: 140px;
        }
    }

    @media (max-width: 768px) {
        .responsive-container {
            flex-direction: column;
            align-items: center;
        }

        .content-container {
            flex-direction: column;
            justify-content: center; /* Center vertically */
            align-items: center;
            width: 80%; /* Adjust as needed */
        }

        .title.is-1 {
            font-size: 1.8rem; /* Smaller font size for mobile */
        }

        .subtitle.is-4 {
            font-size: 1.2rem; /* Smaller font size for mobile */
        }

        .banner-image {
            margin-bottom: 20px; /* Add space between the image and the text */
            width: 40%; /* Full width */
            margin-bottom: 0px;
        }

        .microsoft-authors {
            margin-left: 0px;
            margin-right: 0px;
        }
    }
</style>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <div class="responsive-container">
                        <div class="content-container">
                          <div class="text-container">
                            <h1 class="title is-1" style="word-break: keep-all;">VideoWebArena</h1>
                            <h2 class="subtitle is-4" style="word-break: keep-all;">Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</h2>
                        </div>
                                              
            </div>
            </div>
            <div style="margin-top: 15px;" class="is-size-5 publication-authors microsoft-authors">
              <span class="author-block">
                  <a href="http://lawrencekjang.github.io">Lawrence Jang</a><sup>1,4</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.microsoft.com/applied-sciences/people/yinheng-li">Yinheng Li</a><sup>4</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.linkedin.com/in/charles-dingg/">Charles Ding</a><sup>1</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.linkedin.com/in/justin-lin-68a0831a1/">Justin Lin</a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                  <a href="https://pliang279.github.io/">Paul Liang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                  <a href = "https://www.microsoft.com/applied-sciences/people/danny-zhao"> Dan Zhao</a><sup>2,3,4</sup>,
              </span>
              <span class="author-block">
                  <a href="http://rogeriobonatti.com/">Rogerio Bonatti</a><sup>4</sup>,
              </span>
              <span class="author-block">
                  <a href="https://www.microsoft.com/applied-sciences/people/kazuhito-koishida">Kazuhito Koishida</a><sup>4</sup>
              </span>
              <br>
              <div style="margin-top:5px;">
                  <sup>1</sup>Carnegie Mellon University, <sup>2</sup>Massachusetts Institute of Technology,<br>
                  <sup>3</sup>New York University, <sup>4</sup>Microsoft
              </div>
              <div style="margin-top: 3px; font-family: monospace; color: #888; font-size: 15px;">
                  ljang@cs.cmu.edu
            </div>
          </div>
                  
        <div style="margin-top: 3px;" class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <a href="static/files/windows_agent_arena.pdf"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2409.08264"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/ljang0/videowebarena"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <hr class="light-hr" style="margin-bottom: 0;">

          </div>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser" style="margin-top: 0px; margin-bottom: 0px;  padding-bottom: 0;">
  <div class="container is-max-desktop" style="margin-top: 0px; margin-bottom: 0px;  padding-bottom: 0;">
    <div class="hero-body">
      <img src="./static/images/headerFinal.png" alt="Header Image" style="width: 100%; margin-bottom: 0; ">
    </div>
  </div>
</section>

<section class="section" style="margin-top: 0px; padding-top: 0;">
  <div class="container is-max-desktop" style="margin-top: 0px; padding-top: 0;">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <!-- <h2 class="title is-3">Windows Agent Arena benchmark</h2> -->
        <div class="content has-text-justified">
          <p>Humans often use videos to complete daily tasks, whether to learn from tutorials or retrieve information from within one or several videos. As we build AI assistants, these multimodal agents must also
            possess similar capabilities to understand and process videos to accomplish tasks or learn how to
            accomplish a workflow, plan, and make decisions.</p>
          <p>However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs.</p>
          <p>To address this gap, we present <strong>VideoWebArena</strong>, a novel, open-source video-based benchmark that evaluates multimodal models' agentic ability to process, understand, and utilize long-context video inputs to accomplish various tasks. </p>
          <p>VideoWebArena consists of 2,021 web agent
            tasks based on manually crafted video tutorials, which total almost four hours of
            content. For our benchmark, we define a taxonomy of long-context video-based
            agent tasks with two main areas of focus: skill retention and factual retention.
            While skill retention tasks evaluate whether an agent can use a given human demon-
            stration to complete a task efficiently, the factual retention task evaluates whether
            an agent can retrieve instruction-relevant information from a video to complete
            a task.</p>
          <p>We evaluate several video-capable state-of-the-art LLMs, namely GPT-4o and Gemini 1.5
            Pro, on our benchmark, providing an overview of these models' current long-context video
            understanding capabilities. Our results show that while these models can serve in a limited
            capacity as video-capable agents, these models are still a far reach from human levels of
            performance, highlighting a wide gap in the information retrieval and agentic abilities of
            current state-of-the-art long-context models.</p>
          <p>Our work
            highlights the need to improve the agentic abilities of long-context multimodal
            models and provides a testbed for future development with long-context video
            agents.</p>

          <!-- <p>Code and baseline models: <a href="https://github.com/microsoft/WindowsAgentArena">github.com/microsoft/WindowsAgentArena</a>.</p> -->
        </div>
  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/edge_track.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vscode.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vlc.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bing.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/edge.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/paint.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/main.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section" style="margin-top: 0px;">
  <div class="container is-max-desktop" style="margin-top: 0px;">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Environment and Tasks</h2>
        <div class="content has-text-justified">
          <p>
            VideoWebArena centers around six key thematic environments created by VisualWebArena and WebArena: Reddit, Classifieds, Shopping, Shopping Admin, Map, and Gitlab. These domains' websites are locally hosted as the docker images for each website are publicly available online. There is an Amazon Machine Image and instructions dedicated to hosting these websites on an EC2 instance; we refer readers to the codebase for further information. By doing this, we can make our benchmark realistic and reproducible, leveraging data and code from real and popular websites on the internet. We refer to WebArena and VisualWebArena for more information on each site and their setup.
          </p>
          <p>
            The authors of this paper created 74 unique videos, totaling almost 4 hours of video content ---all of our video tutorials are based on tasks in WebArena and VisualWebArena. We provide our videos online through a <a href = https://www.youtube.com/@webarenawarrior>Youtube Channel</a> and a <a href = https://drive.google.com/file/d/17DwmsM7KzBWyz1BN1aq7NHDvgcTIrCgx/view>Google Drive link</a> containing the zip file of all the videos. We formulated these videos by accumulating all the feasible tasks in WebArena and VisualWebArena, and then filming task tutorials on our personal computers. 
          </p>

          <p>We provide a breakdown of the videos and tasks below.</p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <img src="./static/images/taskNew.png" height="240" alt="Tasks">
    </div>
<!-- 
    <div class="content has-text-justified">

      <p>
        Task evaluation is deterministic, and we use custom scripts to generate a reward at the end of each episode:
      </p>
    </div>

    <div class="content has-text-centered">
      <img src="./static/images/eval.png" height="200" alt="Eval">
    </div>
 -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Video Agent Task Taxonomy: Skill and Factual Retention</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
            We designed the infrastructure behind Windows Agent Arena to support flexible, local execution
during the prototyping phase as well as scalable and secure cloud parallelization in Azure. The core
of our system is a Docker container that hosts the Windows 11 VM. Within the container, we deploy
a client process for task scheduling and configuration as well as the agent and the evaluation scripts.
The VM is our main simulation environment: a Python Flask server acts as the bridge between the
container and the VM by receiving commands from the client processes and executing them within
the VM; it also sends observations and files back to the client.
          </p>
          <p>
            We use Azure Machine Learning jobs to parallelize the benchmark
evaluation using compute instances. The process is similar to the local setup, but the VMs are
instantiated and terminated with each experiment submission. We use the Azure Blob Store to
manage the Windows 11 snapshot and output logs while the code is pre-configured in the Docker
image. Tasks are distributed evenly among the workers, and the results are aggregated at the end of
the run.
          </p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <img src="./static/images/examples_refit.png" height="40" alt="Tasks">
    </div>
    <p>
      We use chain-of-thought prompting to instruct our agent, Navi, to reason about
      the current state of the computer, its own past actions, and decide on the most appropriate next action. 
      Our agent receive as input the title of the current foreground window, titles for all other windows or browser tabs currently open, and a representation of the current screen. We consider several methods to process the screen representation
      for the agent as input and create Set-of-Marks (SoMs):
      </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Baseline Agents</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
          We use chain-of-thought prompting to instruct our agent, Navi, to reason about
          the current state of the computer, its own past actions, and decide on the most appropriate next action. 
          Our agent receive as input the title of the current foreground window, titles for all other windows or browser tabs currently open, and a representation of the current screen. We consider several methods to process the screen representation
          for the agent as input and create Set-of-Marks (SoMs):
          </p>
          <ul>
            <li>UIA tree parsing: extracts the visible elements from the Windows UI Automation tree</li>
            <li>DOM tree parsing: extracts the visible elements from the DOM tree (browser only)</li>
            <li>OCR: proprietary and open models (<a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a>)</li>
            <li>Icon and image detection: proprietary and open models (<a href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a>)</li>
            <li><a href="https://arxiv.org/abs/2408.00203">OmniParser</a>: proprietary model that detects detects text, icons, and images and provides icon captioning</li>
          </ul>
          <p>
          Below you can see a step-by-step example of Navi's reasoning process and screen parsing during a task:
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/agent_refit.png" height="15" alt="Agent Example">
          </div>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Results:</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
          We benchmark several state-of-the-art visual-language model agent configurations. 
          We find that all existing models achieve low performance in comparison to  human behavior, with large variance between domains.
          </p>
          <p>
          The quality of the Set-of-Marks plays a crucial role in the agent's performance. Agents that rely only on pixel-based OCR and icon detection achieve lower performance than those that also use the UIA tree. We also find that Omniparser's icon captioning capability boots performance.
          </p>
        </div>
      <!-- </div>
    </div> -->
    <!--/ Abstract. -->

    <div class="content has-text-centered">
      <!-- insert image -->
      <img src="./static/images/results.png" height="200" alt="Results">
    </div>


  </div>
</section>


<section class="section" id="Citations">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX citation</h2>
    <pre><code>@article{videowebarena,
      title={VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks},
      author={Lawrence Keunho Jang, Yinheng Li, Charles Ding, Justin Lin, Paul Pu Liang, Dan Zhao, Rogerio Bonatti, Kazuhito Koishida},
      journal={arXiv preprint},
      year={2024}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/files/windows_agent_arena.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/microsoft/WindowsAgentArena" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
